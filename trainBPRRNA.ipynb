{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cacaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 1000\n",
      "['id', 'sequence', 'secondary_structure', 'structural_annotation', 'functional_annotation']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/Users/eshitagupta/Study/rna_project/bprna_preprocessed_subset.csv\",\n",
    "    sep=\",\",\n",
    "    engine=\"python\",\n",
    "    quoting=3,        # ignore quotes weirdness\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "print(\"rows:\", len(df))\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0834e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df rows: 1000\n",
      "['id', 'sequence', 'secondary_structure', 'structural_annotation', 'functional_annotation']\n",
      "            id                                           sequence  \\\n",
      "0  bpRNA_CRW_1  ACACAUGCAAGCGAACGUGAUCUCCAGCUUGCUGGGGGAUUAGUGG...   \n",
      "1  bpRNA_CRW_2  AACACAUGCAAGUCGAACGAUGAUCUCCAGCUUGCUGGGGGAUUAG...   \n",
      "2  bpRNA_CRW_3  CGAACGCUGGCGGCGUGCUUAACACAUGCAAGUCGAACGGAAAGGC...   \n",
      "\n",
      "                                 secondary_structure  \\\n",
      "0  .(((.(((..((..((((.(((((.((....))))))))..))))....   \n",
      "1  ..(((.(((..(((..(((((.(((((.((....))))))))).))...   \n",
      "2  (.((((((.(((((((((....(((.(((..(((..(((((..(((...   \n",
      "\n",
      "                               structural_annotation  \\\n",
      "0  ESSSBSSSMMSSBBSSSSBSSSSSBSSHHHHSSSSSSSSBBSSSSB...   \n",
      "1  EESSSBSSSMMSSSBBSSSSSBSSSSSBSSHHHHSSSSSSSSSBSS...   \n",
      "2  SBSSSSSSMSSSSSSSSSIIIISSSBSSSMMSSSBBSSSSSBBSSS...   \n",
      "\n",
      "                               functional_annotation  \n",
      "0  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...  \n",
      "1  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...  \n",
      "2  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...  \n"
     ]
    }
   ],
   "source": [
    "print(\"df rows:\", len(df))\n",
    "print(df.columns.tolist())\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d8677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "OPEN = set(\"([{<\")\n",
    "CLOSE = set(\")]}>\")\n",
    "\n",
    "def normalize_structure_to_3class(db: str):\n",
    "    db = re.sub(r\"\\s+\", \"\", str(db).strip())\n",
    "    out = []\n",
    "    for ch in db:\n",
    "        if ch == '.':\n",
    "            out.append('.')\n",
    "        elif ch in OPEN or ch == '(':\n",
    "            out.append('(')\n",
    "        elif ch in CLOSE or ch == ')':\n",
    "            out.append(')')\n",
    "        else:\n",
    "            # any other symbol (letters/digits/pseudoknot marks) -> unpaired for POC\n",
    "            out.append('.')\n",
    "    return \"\".join(out)\n",
    "\n",
    "def clean_row(seq, db):\n",
    "    seq = re.sub(r\"\\s+\", \"\", str(seq).strip().upper().replace(\"T\", \"U\"))\n",
    "    db3 = normalize_structure_to_3class(db)\n",
    "\n",
    "    # Keep only RNA chars (AUGCN)\n",
    "    if re.search(r\"[^AUGCN]\", seq):\n",
    "        return None\n",
    "\n",
    "    # Now db is guaranteed to be only . ( ) after mapping\n",
    "    if re.search(r\"[^().]\", db3):\n",
    "        return None\n",
    "\n",
    "    if len(seq) != len(db3):\n",
    "        return None\n",
    "\n",
    "    return seq, db3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573a7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "unique db chars: {')', '.', '('}\n",
      "len: 1434 1434\n"
     ]
    }
   ],
   "source": [
    "out = clean_row(df[\"sequence\"].iloc[0], df[\"secondary_structure\"].iloc[0])\n",
    "print(out is None)\n",
    "print(\"unique db chars:\", set(out[1]))\n",
    "print(\"len:\", len(out[0]), len(out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03410fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usable pairs: 989\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for _, r in df.iterrows():\n",
    "    out = clean_row(r[\"sequence\"], r[\"secondary_structure\"])\n",
    "    if out is not None:\n",
    "        pairs.append(out)\n",
    "\n",
    "print(\"usable pairs:\", len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4845c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs <= 500: 989\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 2000\n",
    "pairs_small = [(s, d) for (s, d) in pairs if len(s) <= MAX_LEN]\n",
    "print(\"pairs <= 500:\", len(pairs_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1acb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 890 val: 99\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VOCAB = {'A':0, 'U':1, 'G':2, 'C':3, 'N':4}\n",
    "LAB   = {'.':0, '(':1, ')':2}\n",
    "INV_LAB = {v:k for k,v in LAB.items()}\n",
    "\n",
    "PAD_X = 5      # padding token for sequence\n",
    "PAD_Y = -100   # ignored index for loss\n",
    "\n",
    "class BPRNADataset(Dataset):\n",
    "    def __init__(self, pairs_list):\n",
    "        self.items = pairs_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, db = self.items[idx]\n",
    "        x = torch.tensor([VOCAB.get(ch, VOCAB['N']) for ch in seq], dtype=torch.long)\n",
    "        y = torch.tensor([LAB[ch] for ch in db], dtype=torch.long)\n",
    "        return x, y, seq, db\n",
    "\n",
    "def collate(batch):\n",
    "    xs, ys, seqs, dbs = zip(*batch)\n",
    "    max_len = max(x.size(0) for x in xs)\n",
    "\n",
    "    x_pad = torch.full((len(xs), max_len), PAD_X, dtype=torch.long)\n",
    "    y_pad = torch.full((len(xs), max_len), PAD_Y, dtype=torch.long)\n",
    "    mask  = torch.zeros((len(xs), max_len), dtype=torch.bool)\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        L = x.size(0)\n",
    "        x_pad[i, :L] = x\n",
    "        y_pad[i, :L] = y\n",
    "        mask[i, :L] = True\n",
    "\n",
    "    return x_pad, y_pad, mask, seqs, dbs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use the filtered set for fast POC\n",
    "pairs_used = pairs_small  # or pairs if you want full\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs_used, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(BPRNADataset(train_pairs), batch_size=32, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = DataLoader(BPRNADataset(val_pairs),   batch_size=32, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(\"train:\", len(train_pairs), \"val:\", len(val_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09a640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMDotBracket(nn.Module):\n",
    "    def __init__(self, vocab_size=6, emb=32, hidden=128, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=PAD_X)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden*2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)       # [B,L,emb]\n",
    "        h, _ = self.lstm(e)   # [B,L,2H]\n",
    "        return self.fc(h)     # [B,L,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8e080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMDotBracket(emb=32, hidden=128).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b1b723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotbracket_to_pairs(db: str):\n",
    "    stack = []\n",
    "    pairs = set()\n",
    "    for i, ch in enumerate(db):\n",
    "        if ch == '(':\n",
    "            stack.append(i)\n",
    "        elif ch == ')':\n",
    "            if stack:\n",
    "                j = stack.pop()\n",
    "                pairs.add((j, i))\n",
    "    return pairs\n",
    "\n",
    "def masked_token_accuracy(logits, y_true, mask):\n",
    "    pred = logits.argmax(dim=-1)\n",
    "    correct = ((pred == y_true) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def decode_db(pred_ids, mask):\n",
    "    out = []\n",
    "    for b in range(pred_ids.size(0)):\n",
    "        L = int(mask[b].sum().item())\n",
    "        out.append(\"\".join(INV_LAB[int(i)] for i in pred_ids[b, :L]))\n",
    "    return out\n",
    "\n",
    "def pair_f1(pred_dbs, true_dbs):\n",
    "    tp = fp = fn = 0\n",
    "    for p_db, t_db in zip(pred_dbs, true_dbs):\n",
    "        P = dotbracket_to_pairs(p_db)\n",
    "        T = dotbracket_to_pairs(t_db)\n",
    "        tp += len(P & T)\n",
    "        fp += len(P - T)\n",
    "        fn += len(T - P)\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    f1   = 2*prec*rec / max(prec + rec, 1e-12)\n",
    "    return prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9efda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.9845 acc 0.4983 | val loss 0.9267 acc 0.5492 | pairF1 0.0059 (P 0.0071, R 0.0051)\n",
      "Epoch 02 | train loss 0.7923 acc 0.6367 | val loss 0.6497 acc 0.7274 | pairF1 0.0629 (P 0.0638, R 0.0621)\n",
      "Epoch 03 | train loss 0.4910 acc 0.8013 | val loss 0.3909 acc 0.8437 | pairF1 0.1616 (P 0.1637, R 0.1596)\n",
      "Epoch 04 | train loss 0.2783 acc 0.8998 | val loss 0.2212 acc 0.9286 | pairF1 0.4385 (P 0.4407, R 0.4364)\n",
      "Epoch 05 | train loss 0.1531 acc 0.9545 | val loss 0.1302 acc 0.9644 | pairF1 0.6053 (P 0.6073, R 0.6033)\n",
      "Epoch 06 | train loss 0.0909 acc 0.9772 | val loss 0.0835 acc 0.9802 | pairF1 0.7761 (P 0.7791, R 0.7731)\n",
      "Epoch 07 | train loss 0.0610 acc 0.9860 | val loss 0.0629 acc 0.9842 | pairF1 0.8520 (P 0.8568, R 0.8473)\n",
      "Epoch 08 | train loss 0.0468 acc 0.9885 | val loss 0.0487 acc 0.9873 | pairF1 0.8694 (P 0.8730, R 0.8658)\n",
      "Epoch 09 | train loss 0.0415 acc 0.9895 | val loss 0.0561 acc 0.9846 | pairF1 0.8519 (P 0.8574, R 0.8465)\n",
      "Epoch 10 | train loss 0.1204 acc 0.9622 | val loss 0.0775 acc 0.9814 | pairF1 0.8317 (P 0.8346, R 0.8288)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    tr_loss = tr_acc = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y, mask, seqs, dbs in train_loader:\n",
    "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)  # [B,L,3]\n",
    "        loss = loss_fn(logits.reshape(-1, 3), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_acc  += masked_token_accuracy(logits, y, mask)\n",
    "        n += 1\n",
    "\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    va_loss = va_acc = 0.0\n",
    "    all_pred_db, all_true_db = [], []\n",
    "    m = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask, seqs, dbs in val_loader:\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.reshape(-1, 3), y.reshape(-1))\n",
    "\n",
    "            va_loss += loss.item()\n",
    "            va_acc  += masked_token_accuracy(logits, y, mask)\n",
    "\n",
    "            pred_ids = logits.argmax(dim=-1).cpu()\n",
    "            all_pred_db.extend(decode_db(pred_ids, mask.cpu()))\n",
    "            all_true_db.extend(dbs)\n",
    "            m += 1\n",
    "\n",
    "    p, r, f1 = pair_f1(all_pred_db, all_true_db)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss {tr_loss/n:.4f} acc {tr_acc/n:.4f} | \"\n",
    "        f\"val loss {va_loss/m:.4f} acc {va_acc/m:.4f} | \"\n",
    "        f\"pairF1 {f1:.4f} (P {p:.4f}, R {r:.4f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f87859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs_used: 989\n",
      "train_pairs: 890 val_pairs: 99\n",
      "train batches: 28 val batches: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"pairs_used:\", len(pairs_used))\n",
    "print(\"train_pairs:\", len(train_pairs), \"val_pairs:\", len(val_pairs))\n",
    "print(\"train batches:\", len(train_loader), \"val batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5030d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0\n",
      "TRUE: (((.(((..(((..(((((.((((((((....)))))))))).))))))......(((......((((((((..((...(((((((.((((....(((((((....))))))).....))\n",
      "PRED: (((((((....(..)))...(((((((((.....))))))))..)))))......(((......((((((((..((...(((((((.((((....(((((((....))))))).....))\n",
      "\n",
      "Example 1\n",
      "TRUE: ((.((((((.(((((((((....(((.(((..(((..(((((.((((((....)))))))).))))))......(((......((((((((..((...(((((((.((((....((((((\n",
      "PRED: ((.((((((.(((((((((....(((.(((..(((..(((((.((((((....)))))))).))))))......(((......((((((((..((...(((((((.((((....((((((\n",
      "\n",
      "Example 2\n",
      "TRUE: ((((....(((.(((..(((..(((((.((((((....)))))))).))))))......(((......((((((((..((...(((((((.((((....(((((((....)))))))...\n",
      "PRED: (((.....(((.(((..(((..(((((.((((((....)))))))).))))))......(((......((((((((..((...(((((((.((((....(((((((....)))))))...\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"\\nExample\", i)\n",
    "    print(\"TRUE:\", all_true_db[i][:120])\n",
    "    print(\"PRED:\", all_pred_db[i][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f590d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_parentheses(db):\n",
    "    db = list(db)\n",
    "    stack = 0\n",
    "    for i, ch in enumerate(db):\n",
    "        if ch == '(':\n",
    "            stack += 1\n",
    "        elif ch == ')':\n",
    "            if stack == 0:\n",
    "                db[i] = '.'   # remove invalid close\n",
    "            else:\n",
    "                stack -= 1\n",
    "    # remove extra opens from the end\n",
    "    for i in range(len(db)-1, -1, -1):\n",
    "        if stack == 0:\n",
    "            break\n",
    "        if db[i] == '(':\n",
    "            db[i] = '.'\n",
    "            stack -= 1\n",
    "    return \"\".join(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d753c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_db = [balance_parentheses(s) for s in decode_db(pred_ids, mask.cpu())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90b868a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bprna_layer1_bilstm.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"bprna_layer1_bilstm.pt\")\n",
    "print(\"saved bprna_layer1_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2920d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
